{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd9bad-fad2-458e-ad8b-07de87879223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully.\n",
      "Page 2 scraped successfully.\n",
      "Page 3 scraped successfully.\n",
      "Page 4 scraped successfully.\n",
      "Page 5 scraped successfully.\n",
      "Page 6 scraped successfully.\n",
      "Page 7 scraped successfully.\n",
      "Page 8 scraped successfully.\n",
      "Page 9 scraped successfully.\n",
      "Page 10 scraped successfully.\n",
      "Page 11 scraped successfully.\n",
      "Page 12 scraped successfully.\n",
      "Page 13 scraped successfully.\n",
      "Page 14 scraped successfully.\n",
      "Page 15 scraped successfully.\n",
      "Page 16 scraped successfully.\n",
      "Page 17 scraped successfully.\n",
      "Page 18 scraped successfully.\n",
      "Page 19 scraped successfully.\n",
      "Page 20 scraped successfully.\n",
      "Page 21 scraped successfully.\n",
      "Page 22 scraped successfully.\n",
      "Page 23 scraped successfully.\n",
      "Page 24 scraped successfully.\n",
      "Page 25 scraped successfully.\n",
      "Page 26 scraped successfully.\n",
      "Page 27 scraped successfully.\n",
      "Page 28 scraped successfully.\n",
      "Page 29 scraped successfully.\n",
      "Page 30 scraped successfully.\n",
      "Page 31 scraped successfully.\n",
      "Page 32 scraped successfully.\n",
      "Page 33 scraped successfully.\n",
      "Page 34 scraped successfully.\n",
      "Page 35 scraped successfully.\n",
      "Page 36 scraped successfully.\n",
      "Page 37 scraped successfully.\n",
      "Page 38 scraped successfully.\n",
      "Page 39 scraped successfully.\n",
      "Page 40 scraped successfully.\n",
      "Page 41 scraped successfully.\n",
      "Page 42 scraped successfully.\n",
      "Page 43 scraped successfully.\n",
      "Page 44 scraped successfully.\n",
      "Page 45 scraped successfully.\n",
      "Page 46 scraped successfully.\n",
      "Page 47 scraped successfully.\n",
      "Page 48 scraped successfully.\n",
      "Page 49 scraped successfully.\n",
      "Page 50 scraped successfully.\n",
      "Page 51 scraped successfully.\n",
      "Page 52 scraped successfully.\n",
      "Page 53 scraped successfully.\n",
      "Page 54 scraped successfully.\n",
      "Page 55 scraped successfully.\n",
      "Page 56 scraped successfully.\n",
      "Page 57 scraped successfully.\n",
      "Page 58 scraped successfully.\n",
      "Page 59 scraped successfully.\n",
      "Page 60 scraped successfully.\n",
      "Page 61 scraped successfully.\n",
      "Page 62 scraped successfully.\n",
      "Page 63 scraped successfully.\n",
      "Page 64 scraped successfully.\n",
      "Page 65 scraped successfully.\n",
      "Page 66 scraped successfully.\n",
      "Page 67 scraped successfully.\n",
      "Page 68 scraped successfully.\n",
      "Page 69 scraped successfully.\n",
      "Page 70 scraped successfully.\n",
      "Page 71 scraped successfully.\n",
      "Page 72 scraped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Title: Hockey Team Data Scraping\n",
    "# Name: Daniel Muthama\n",
    "# Date: 18 may 2025\n",
    "# Description: Extracts multi-page hockey team data into a structured CSV.\n",
    "\n",
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configure headers and base URL\n",
    "base_url = 'https://www.scrapethissite.com/pages/forms/'\n",
    "http_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}  # Renamed to http_headers\n",
    "\n",
    "# Initialize DataFrame with column headers\n",
    "response = requests.get(base_url, headers=http_headers)  # Use http_headers here\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Corrected: Use a different variable name for table headers\n",
    "column_headers = [th.text.strip() for th in table.find_all('th')]  # Renamed to column_headers\n",
    "df = pd.DataFrame(columns=column_headers)\n",
    "\n",
    "# Loop through all pages (pagination handling)\n",
    "page_num = 1\n",
    "while True:\n",
    "    # Build URL for each page\n",
    "    url = base_url if page_num == 1 else f\"{base_url}?page_num={page_num}\"\n",
    "    response = requests.get(url, headers=http_headers)  # Use http_headers here\n",
    "    \n",
    "    # Break loop if page fetch fails\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Stopped at page {page_num}. Status code: {response.status_code}\")\n",
    "        break\n",
    "    \n",
    "    # Parse HTML and extract table rows\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "    if not table:\n",
    "        print(f\"No table found on page {page_num}. Exiting loop.\")\n",
    "        break\n",
    "    \n",
    "    # Extract and clean data rows\n",
    "    rows = table.find_all('tr')[1:]  # Skip header row\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        row_data = [col.text.strip() for col in cols]\n",
    "        if len(row_data) == len(df.columns):\n",
    "            df.loc[len(df)] = row_data  # Append to DataFrame\n",
    "    \n",
    "    print(f\"Page {page_num} scraped successfully.\")\n",
    "    page_num += 1\n",
    "    time.sleep(1)  # Avoid overwhelming the server\n",
    "\n",
    "# Clean numeric columns (example: \"Wins\" and \"Losses\")\n",
    "df['Wins'] = df['Wins'].astype(int)\n",
    "df['Losses'] = df['Losses'].astype(int)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('hockey_teams.csv', index=False)\n",
    "print(\"Data exported to hockey_teams.csv\")\n",
    "\n",
    "# Display preview\n",
    "print(\"\\nPreview of the DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec504cc-4b3f-4725-b3ea-92cce4183b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
